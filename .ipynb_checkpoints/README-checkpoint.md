# Sparkify Data Modelling with Postgres

### Project Description
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. As a Data Engineer, we have to create a Postgres database schema and ETL pipeline for this analysis.

### Project Overview
In this project, we have performed the extract, transform, load on the following tables from the Sparkify app logs(a music streaming app to listen songs):

- `songs`
- `artists`
- `users`
- `time`
- `Songplays`

We are using this star schema data structure for the database. We can extract meaningful and analytical information about the users and their patterns of using the app.

### Sparkify Dataset

**Song Dataset:** This dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song.



**Log Dataset:** This dataset is a subset of real dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.


>{
"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0
>}

### Database Schema Design

Here I have used a Star Schema Data structure for the database. The database contains 4 dimension tables and with each primary key referenced to the fact table as a foreign key.

#### Users table
- **Name**: users | **Type**: Dimension table
| Column     | Type                |
|------------|---------------------|
| user_id    | integer primary key |
| first_name | varchar not null    |
| last_name  | varchar not null    |
| gender     | char                |
| level      | varchar not null    |

#### Songs table
- **Name**: songs | **Type**: Dimension table

| Column    | Type                                               |
|-----------|----------------------------------------------------|
| song_id   | varchar primary key                                |
| title     | varchar not null                                   |
| artist_id | varchar(30) not null references artists(artist_id) |
| year      | integer not null                                   |
| duration  | numeric(14,5) not null                             |

### Artists table
- **Name**: artist | **Type**: Dimension table

| Column    | Type                |
|-----------|---------------------|
| artist_id | varchar primary key |
| name      | varchar not null    |
| location  | varchar             |
| latitude  | numeric             |
| longitude | numeric             |

### Time table
It is an auxiliary which table helps to keep the break up of timestamps in columns such as day, weekday, month etc.

- **Name**: time | **Type**: Dimension table

| Column     | Type                           |
|------------|--------------------------------|
| start_time | timestamp not null primary key |
| hour       | numeric not null               |
| day        | numeric not null               |
| week       | numeric not null               |
| month      | numeric not null               |
| year       | numeric                        |
| weekday    | numeric not null               |

### Songplays table
- **Name**: songplays | **Type**: Fact table

| Column      | Type                                       |
|-------------|--------------------------------------------|
| songplay_id | integer                                    |
| start_time  | timestamp references time(start_time)      |
| user_id     | integer not null references users(user_id) |
| level       | varchar(25)                                |
| song_id     | varchar(25) references songs(song_id)      |
| artist_id   | varchar(25) references artists(artist_id)  |
| session_id  | integer not null                           |
| location    | varchar(60)                                |
| user_agent  | varchar(150)                               |

## ETL files

- First, we have to create a Database Schema on a Postgres database for which we have to run `python create_table.py`.

- Then, we have to parse both the song and log data files in json format. This step also involves Data Wrangling, Data Cleaning and populating the records into the tables in the database.\
 For this we have to run `python etl.py`.

## ETL pipeline

- After Creating the Postgres database schema and table structure by running create_tables.py, we will run etl.py.

- In etl.py the `main()` function is called which will call the **process_data()** twice once for song_data and then for log_data.

```
process_data(cur, conn, filepath='data/song_data', func=process_song_file)
process_data(cur, conn, filepath='data/log_data', func=process_log_file)
```
- `processs_data()` will walk through the directory in which all the songs and logs json data is stored.
  - It will store all the JSON files in a list.
  - Further, we will iterate all the files each time and pass it into the function `process_song_file` and `process_song_file`.

  ```
  # iterate over files and process
    for i, datafile in enumerate(all_files, 1):
        func(cur, datafile)
        conn.commit()
        print('{}/{} files processed.'.format(i, num_files))
  ```

- In the `process_song_file` we read the json files, partitioned the data frame, cleaned the data frame and transformed the records into list format.
  - Finally, we will run `cur.execute()` whose basic functionality is to run the sql query and arguments(records) passed as the 2nd argument.
  - This, will populate the tables.

- Similarly, in the `process_log_file()` we read the json files, performed data wrangling, cleaned the data frame.
  - Now, here for inserting data into **time table** we will iterate through the records in the dataframe and pass those records in the list format as `iterrows()` will generate records in series format.

  ```
  # insert time data records
    for i, row in t_s.iterrows():
        cur.execute(time_table_insert, list(row))
  ```

  - For **user table** I am passing records in the series format into `cur.execute()` as the second argument.

  - For the songplay table, we will first get songid and artistid from the original log data frame and pass those values into the `song_select` sql query.

  - Finally, the songplay records will be passed as a tuple into the `cur.execute()`.

  ```
  # insert songplay record
      songplay_data = (
          index,
          pd.to_datetime(row.ts, unit='ms'),
          row.userId,
          row.level,
          songid,
          artistid,
          row.sessionId,
          row.location,
          row.userAgent
      )
      cur.execute(songplay_table_insert, songplay_data)
  ```
